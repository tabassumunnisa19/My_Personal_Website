<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Machine Learning Interview Questions — Nisa</title>

<!-- Main Site CSS -->
<link rel="stylesheet" href="../style.css">

<!-- Font Awesome -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<style>
.topic-header{
  width:100%;
  padding:1.25rem 1.6rem;
  margin:2.5rem 0 2rem;
  border-radius:16px;
  font-size:1.45rem;
  font-weight:700;
  background:#e0ecff;
  color:#1e3a8a;
  box-shadow:0 8px 18px rgba(0,0,0,0.08);
}

.qa-section{margin-bottom:3.5rem}

.qa-block{
  border-radius:14px;
  margin-bottom:1.4rem;
  overflow:hidden;
  background:#eef2ff;
  box-shadow:0 6px 16px rgba(0,0,0,0.08);
}

.qa-block button{
  width:100%;
  border:none;
  background:#c7d2fe;
  color:#1e3a8a;
  cursor:pointer;
  font-size:1rem;
  padding:1.05rem 1.4rem;
  display:flex;
  justify-content:space-between;
  align-items:center;
  font-weight:600;
}

.qa-block button:hover{background:#a5b4fc}

.qa-block .content{
  display:none;
  padding:1.2rem 1.5rem;
  background:#ffffff;
  border-top:1px solid #c7d2fe;
  font-size:0.95rem;
  line-height:1.65;
  color:#1e293b;
}
</style>
</head>

<body>

<!-- ================= HEADER ================= -->
<header class="site-header">
  <div class="site-container header-inner">
    <a href="../index.html" class="logo">
      <img src="../logo.jpg" alt="Nisa logo">
    </a>

    <button class="menu-toggle" aria-label="Toggle menu">
      <span class="bar"></span><span class="bar"></span><span class="bar"></span>
    </button>

    <nav class="navbar">
      <a href="../index.html">Home</a>
      <a href="../about.html">About</a>
      <a href="../portfolio.html">Portfolio</a>
      <a href="../learn.html" class="active">Learn</a>
      <a href="../blog.html">Blog</a>
      <a href="../books.html">Books</a>
      <a href="../contact.html">Contact</a>
    </nav>
  </div>
</header>

<!-- ================= MAIN CONTENT ================= -->
<main class="site-container" style="padding:2rem;">

<h2 class="topic-header">
  Topic — Machine Learning Fundamentals
</h2>

<div class="qa-section">

<div class="qa-block">
<button>1. What is Machine Learning in real-world terms? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Machine Learning is the ability of systems to learn patterns from historical data
and make predictions or decisions on new, unseen data without explicit rules.
<br><br>
<strong>Interview focus:</strong> Prediction + learning from data.
</div>
</div>

<div class="qa-block">
<button>2. How is Machine Learning different from traditional programming? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Traditional programming uses predefined rules written by humans.
Machine Learning learns rules automatically from data.
<br><br>
<strong>Example:</strong> Spam detection adapts continuously — rules cannot.
</div>
</div>

<div class="qa-block">
<button>3. How is Machine Learning different from statistics? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Statistics focuses on inference and explanation.
Machine Learning focuses on prediction accuracy and scalability.
<br><br>
<strong>Interview trap:</strong> “ML is advanced statistics” ❌
</div>
</div>

<div class="qa-block">
<button>4. When should you NOT use Machine Learning? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Do NOT use ML when:
<ul>
<li>Simple rules are sufficient</li>
<li>Very little data exists</li>
<li>No learnable pattern exists</li>
<li>Business logic changes daily</li>
</ul>
<strong>This answer scores HIGH in interviews.</strong>
</div>
</div>

<div class="qa-block">
<button>5. What are the main types of Machine Learning? <i class="fas fa-chevron-down"></i></button>
<div class="content">
<ul>
<li><strong>Supervised</strong> — labeled data</li>
<li><strong>Unsupervised</strong> — unlabeled data</li>
<li><strong>Semi-supervised</strong></li>
<li><strong>Reinforcement Learning</strong></li>
</ul>
</div>
</div>

<div class="qa-block">
<button>6. What makes a problem suitable for Machine Learning? <i class="fas fa-chevron-down"></i></button>
<div class="content">
A problem is suitable if:
<ul>
<li>Historical data is available</li>
<li>Patterns exist</li>
<li>Predictions create business value</li>
<li>Some error is acceptable</li>
</ul>
</div>
</div>

<div class="qa-block">
<button>7. What is data leakage and why is it dangerous? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Data leakage happens when future or target information
is used during training, leading to unrealistically high accuracy
and complete failure in production.
</div>
</div>

<div class="qa-block">
<button>8. Can a model be accurate but still useless? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Yes. Accuracy alone ignores business context.
In imbalanced datasets, predicting the majority class
gives high accuracy but no value.
<br><br>
<strong>Example:</strong> Fraud detection.
</div>
</div>

<div class="qa-block">
<button>9. Why do Machine Learning models fail in production? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Common reasons:
<ul>
<li>Data drift</li>
<li>Overfitting</li>
<li>Poor data quality</li>
<li>Wrong metric optimization</li>
<li>Changing user behavior</li>
</ul>
</div>
</div>

<div class="qa-block">
<button>10. What is generalization in Machine Learning? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Generalization is a model’s ability to perform well
on unseen data, not just training data.
</div>
</div>
  
<!-- ================= MAIN CONTENT ================= -->
<main class="site-container" style="padding:2rem;">

<h2 class="topic-header">
  Topic — Supervised Learning (Regression & Classification)
</h2>

<div class="qa-section">

<div class="qa-block">
<button>1. What is supervised learning in simple terms? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Supervised learning is a type of machine learning where the model learns
from labeled data — meaning each input has a known output.
<br><br>
<strong>Interview expectation:</strong> Input → Output mapping using labeled examples.
</div>
</div>

<div class="qa-block">
<button>2. How do you decide whether a problem is regression or classification? <i class="fas fa-chevron-down"></i></button>
<div class="content">
If the target variable is continuous (price, temperature, revenue),
it is a regression problem.
If the target variable is categorical (yes/no, churn/not churn),
it is a classification problem.
</div>
</div>

<div class="qa-block">
<button>3. Why should you always start with a baseline model? <i class="fas fa-chevron-down"></i></button>
<div class="content">
A baseline model provides a reference point.
It helps measure whether complex models actually add value
or just increase complexity.
<br><br>
<strong>Interview insight:</strong> Jumping directly to advanced models is a red flag.
</div>
</div>

<div class="qa-block">
<button>4. Linear Regression vs Decision Tree — when do you use each? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Use Linear Regression when:
<ul>
<li>Relationship is approximately linear</li>
<li>Interpretability is important</li>
</ul>
Use Decision Trees when:
<ul>
<li>Data has non-linear relationships</li>
<li>Rules and thresholds matter</li>
</ul>
</div>
</div>

<div class="qa-block">
<button>5. When does Linear Regression fail completely? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Linear Regression fails when:
<ul>
<li>Strong non-linearity exists</li>
<li>Outliers dominate</li>
<li>Multicollinearity is high</li>
<li>Homoscedasticity assumption breaks</li>
</ul>
</div>
</div>

<div class="qa-block">
<button>6. Why is Logistic Regression still widely used? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Logistic Regression is:
<ul>
<li>Simple and fast</li>
<li>Highly interpretable</li>
<li>Good for probability estimation</li>
</ul>
<strong>Real use:</strong> Credit approval, medical diagnosis.
</div>
</div>

<div class="qa-block">
<button>7. What does the output of Logistic Regression represent? <i class="fas fa-chevron-down"></i></button>
<div class="content">
The output is a probability between 0 and 1
representing the likelihood of belonging to a class.
A threshold converts it into a class label.
</div>
</div>

<div class="qa-block">
<button>8. Can Logistic Regression handle non-linear data? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Not directly.
However, non-linearity can be introduced
using feature engineering or polynomial features.
</div>
</div>

<div class="qa-block">
<button>9. How do business constraints affect algorithm choice? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Business constraints such as explainability,
latency, regulation, and cost
often matter more than raw accuracy.
</div>
</div>

<div class="qa-block">
<button>10. Can the same dataset be modeled as both regression and classification? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Yes.
Example: Predicting exact sales value (regression)
vs predicting high/low sales category (classification).
</div>
</div>

<div class="qa-block">
<button>11. What happens if the target variable contains outliers? <i class="fas fa-chevron-down"></i></button>
<div class="content">
Outliers can heavily distort regression models.
Solutions include:
<ul>
<li>Outlier removal</li>
<li>Robust models</li>
<li>Transformation of target variable</li>
</ul>
</div>
</div>

<div class="qa-block">
<button>12. What is the cost of a wrong prediction in real applications? <i class="fas fa-chevron-down"></i></button>
<div class="content">
The cost depends on the problem.
False negatives may be worse than false positives
in fraud or medical use cases.
<br><br>
<strong>Interviewers expect business thinking here.</strong>
</div>
</div>

</div>
<!-- =================================================
     TOPIC — TREE-BASED ALGORITHMS
================================================= -->
<h2 class="topic-header topic-purple">
  Topic — Tree-Based Algorithms (Decision Tree, Random Forest, Boosting)
</h2>

<div class="qa-section">

  <div class="qa-block">
    <button>1. What is a Decision Tree in machine learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      A Decision Tree is a model that makes predictions by splitting data into branches
      based on feature conditions, forming a tree-like structure of decisions.
      It closely mimics human decision-making and is easy to interpret.
    </div>
  </div>

  <div class="qa-block">
    <button>2. How does a Decision Tree decide where to split the data? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      It selects splits that best reduce impurity using measures such as
      Gini Index or Entropy. The goal is to create child nodes that are as
      homogeneous as possible.
    </div>
  </div>

  <div class="qa-block">
    <button>3. Why do Decision Trees overfit easily? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Decision Trees can grow very deep and learn noise in the training data.
      They keep splitting until training accuracy is maximized, leading to high variance.
    </div>
  </div>

  <div class="qa-block">
    <button>4. What is pruning and why is it important? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Pruning removes unnecessary branches from a tree to reduce overfitting,
      improve generalization, and simplify the model.
    </div>
  </div>

  <div class="qa-block">
    <button>5. Gini Index vs Entropy — which one do you prefer and why? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Both measure node impurity and usually give similar results.
      Gini Index is computationally faster and is commonly preferred in practice.
    </div>
  </div>

  <div class="qa-block">
    <button>6. What is Random Forest and how is it different from a Decision Tree? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Random Forest is an ensemble of multiple Decision Trees trained on
      random subsets of data and features. This reduces overfitting and
      improves prediction stability.
    </div>
  </div>

  <div class="qa-block">
    <button>7. Why does Random Forest perform better than a single tree? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Random Forest reduces variance by combining multiple independent trees.
      Individual errors cancel out, making the overall model more robust.
    </div>
  </div>

  <div class="qa-block">
    <button>8. Can Random Forest overfit? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Yes, but it is much less prone to overfitting than a single tree.
      Overfitting can still occur with very deep trees or very small datasets.
    </div>
  </div>

  <div class="qa-block">
    <button>9. What is feature importance in tree-based models? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Feature importance indicates how much each feature contributes
      to reducing impurity across all splits in the model.
    </div>
  </div>

  <div class="qa-block">
    <button>10. Random Forest vs Gradient Boosting — when do you use which? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Random Forest is preferred for stability and minimal tuning.
      Gradient Boosting is chosen when higher accuracy is needed
      and careful tuning is possible.
    </div>
  </div>

  <div class="qa-block">
    <button>11. Why is boosting more sensitive to noise? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Boosting focuses on correcting previous errors.
      If the data contains noise, the model may keep trying to fit it,
      leading to overfitting.
    </div>
  </div>

  <div class="qa-block">
    <button>12. When should you avoid tree-based models? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Tree-based models should be avoided when the dataset is very small,
      relationships are strictly linear, or strong extrapolation is required.
    </div>
  </div>

</div>
<!-- =================================================
     TOPIC — KNN, NAIVE BAYES & SVM
================================================= -->
<h2 class="topic-header topic-teal">
  Topic — KNN, Naive Bayes & SVM
</h2>

<div class="qa-section">

  <div class="qa-block">
    <button>1. What is K-Nearest Neighbors (KNN)? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      KNN is a distance-based algorithm that predicts outcomes by looking at the
      closest K data points in the feature space. It makes no assumptions about
      data distribution and learns at prediction time.
    </div>
  </div>

  <div class="qa-block">
    <button>2. When should you use KNN in real applications? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      KNN is suitable when the dataset is small to medium-sized,
      feature space is low-dimensional, and interpretability of neighbors matters.
    </div>
  </div>

  <div class="qa-block">
    <button>3. Why does KNN fail on large datasets? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      KNN requires computing distances to many points during prediction,
      making it computationally expensive and slow for large datasets.
    </div>
  </div>

  <div class="qa-block">
    <button>4. Why is feature scaling important for KNN? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      KNN relies on distance metrics.
      Features with larger scales can dominate distance calculations,
      leading to biased predictions if scaling is not applied.
    </div>
  </div>

  <div class="qa-block">
    <button>5. What is Naive Bayes and why is it called “naive”? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Naive Bayes is a probabilistic classifier based on Bayes’ theorem.
      It is called “naive” because it assumes features are independent,
      which is rarely true in real data.
    </div>
  </div>

  <div class="qa-block">
    <button>6. Why does Naive Bayes work well despite its strong assumptions? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Even when independence assumptions are violated,
      relative probability estimates remain effective,
      making Naive Bayes surprisingly accurate in many cases.
    </div>
  </div>

  <div class="qa-block">
    <button>7. What real-world problems suit Naive Bayes best? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Naive Bayes works well for text classification tasks such as
      spam detection, sentiment analysis, and document categorization.
    </div>
  </div>

  <div class="qa-block">
    <button>8. What is Support Vector Machine (SVM)? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      SVM is a supervised algorithm that finds an optimal hyperplane
      separating classes by maximizing the margin between them.
    </div>
  </div>

  <div class="qa-block">
    <button>9. Why is SVM effective in high-dimensional spaces? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      SVM focuses on support vectors rather than the full dataset,
      making it robust and effective in high-dimensional feature spaces.
    </div>
  </div>

  <div class="qa-block">
    <button>10. What is the kernel trick in SVM? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      The kernel trick allows SVM to solve non-linear problems
      by implicitly mapping data into a higher-dimensional space
      without explicitly computing the transformation.
    </div>
  </div>

  <div class="qa-block">
    <button>11. KNN vs SVM — when do you choose which? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Choose KNN for small datasets with simple patterns.
      Choose SVM for complex decision boundaries and high-dimensional data,
      especially when accuracy is critical.
    </div>
  </div>

  <div class="qa-block">
    <button>12. When should you avoid KNN, Naive Bayes, and SVM? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Avoid these models when datasets are extremely large,
      require real-time predictions with low latency,
      or when interpretability and scalability are top priorities.
    </div>
  </div>

</div>
<!-- =================================================
     TOPIC — UNSUPERVISED LEARNING (CLUSTERING)
================================================= -->
<h2 class="topic-header topic-orange">
  Topic — Unsupervised Learning (Clustering)
</h2>

<div class="qa-section">

  <div class="qa-block">
    <button>1. What is unsupervised learning in machine learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Unsupervised learning deals with data that has no labeled outcomes.
      The goal is to discover hidden patterns, structures, or groupings
      directly from the data.
    </div>
  </div>

  <div class="qa-block">
    <button>2. When do you choose unsupervised learning over supervised learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Unsupervised learning is chosen when labels are unavailable,
      expensive to obtain, or when the goal is exploration rather than prediction.
    </div>
  </div>

  <div class="qa-block">
    <button>3. What is clustering and why is it useful? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Clustering groups similar data points together based on feature similarity.
      It is useful for segmentation, pattern discovery, and exploratory analysis.
    </div>
  </div>

  <div class="qa-block">
    <button>4. What is K-Means clustering? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      K-Means is a centroid-based clustering algorithm that partitions data
      into K clusters by minimizing the distance between points and their cluster centroids.
    </div>
  </div>

  <div class="qa-block">
    <button>5. Why does K-Means require the number of clusters (K) beforehand? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      K-Means assumes a fixed number of clusters.
      The algorithm optimizes cluster centroids based on this value,
      making K a critical hyperparameter.
    </div>
  </div>

  <div class="qa-block">
    <button>6. How do you decide the optimal value of K? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Common techniques include the Elbow Method,
      Silhouette Score, and domain knowledge.
      There is no single universally correct value.
    </div>
  </div>

  <div class="qa-block">
    <button>7. Why does K-Means fail in real-world data sometimes? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      K-Means assumes spherical clusters of similar size
      and is sensitive to outliers and feature scaling,
      which often do not hold in real data.
    </div>
  </div>

  <div class="qa-block">
    <button>8. What is hierarchical clustering? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Hierarchical clustering builds a tree-like structure (dendrogram)
      showing nested clusters, without requiring a predefined number of clusters.
    </div>
  </div>

  <div class="qa-block">
    <button>9. When is hierarchical clustering preferred over K-Means? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      It is preferred when the dataset is small to medium-sized,
      cluster relationships matter, and interpretability is important.
    </div>
  </div>

  <div class="qa-block">
    <button>10. What is DBSCAN and why is it different from K-Means? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      DBSCAN is a density-based clustering algorithm
      that can find arbitrarily shaped clusters
      and automatically identify noise and outliers.
    </div>
  </div>

  <div class="qa-block">
    <button>11. How do you evaluate clustering results without labels? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Clustering can be evaluated using internal metrics such as
      Silhouette Score, Davies–Bouldin Index,
      and by validating results with domain knowledge.
    </div>
  </div>

  <div class="qa-block">
    <button>12. What are real-world use cases of clustering? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Common applications include customer segmentation,
      market basket analysis, anomaly detection,
      image segmentation, and recommendation systems.
    </div>
  </div>

</div>
<!-- =================================================
     TOPIC — DIMENSIONALITY REDUCTION (PCA)
================================================= -->
<h2 class="topic-header topic-indigo">
  Topic — Dimensionality Reduction (PCA)
</h2>

<div class="qa-section">

  <div class="qa-block">
    <button>1. What is dimensionality reduction in machine learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Dimensionality reduction is the process of reducing the number of input features
      while preserving as much important information as possible.
      It helps simplify models and improve efficiency.
    </div>
  </div>

  <div class="qa-block">
    <button>2. Why is dimensionality reduction needed in real-world datasets? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Real-world datasets often have many correlated or redundant features.
      High dimensionality increases computation cost, overfitting risk,
      and makes models harder to interpret.
    </div>
  </div>

  <div class="qa-block">
    <button>3. What is the curse of dimensionality? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      As dimensions increase, data points become sparse and distance-based
      methods lose effectiveness, making learning and generalization harder.
    </div>
  </div>

  <div class="qa-block">
    <button>4. What is PCA (Principal Component Analysis)? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      PCA is a linear dimensionality reduction technique that transforms
      original features into a new set of uncorrelated variables
      called principal components, ordered by explained variance.
    </div>
  </div>

  <div class="qa-block">
    <button>5. What does PCA actually optimize? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      PCA maximizes variance captured in the data while ensuring
      principal components are orthogonal to each other.
    </div>
  </div>

  <div class="qa-block">
    <button>6. Why is feature scaling required before PCA? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      PCA is variance-based.
      Features with larger scales can dominate the principal components
      if data is not standardized.
    </div>
  </div>

  <div class="qa-block">
    <button>7. How do you decide how many principal components to keep? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Common approaches include explained variance ratio,
      scree plots, and retaining components that capture
      a predefined percentage of total variance (e.g., 90–95%).
    </div>
  </div>

  <div class="qa-block">
    <button>8. Does PCA always improve model accuracy? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      No.
      PCA may reduce noise and overfitting, but it can also
      remove useful information, sometimes decreasing accuracy.
    </div>
  </div>

  <div class="qa-block">
    <button>9. PCA vs feature selection — what is the difference? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      PCA creates new transformed features.
      Feature selection keeps a subset of original features.
      PCA improves efficiency; feature selection preserves interpretability.
    </div>
  </div>

  <div class="qa-block">
    <button>10. Why does PCA reduce interpretability? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Principal components are combinations of original features,
      making it difficult to explain them in business terms.
    </div>
  </div>

  <div class="qa-block">
    <button>11. When should you avoid PCA? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Avoid PCA when interpretability is critical,
      features are already meaningful, or when the dataset
      has very few features.
    </div>
  </div>

  <div class="qa-block">
    <button>12. What are real-world applications of PCA? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      PCA is commonly used in image compression,
      noise reduction, bioinformatics,
      finance risk modeling, and exploratory data analysis.
    </div>
  </div>

</div>
<!-- =================================================
     TOPIC — FEATURE ENGINEERING
================================================= -->
<h2 class="topic-header topic-blue">
  Topic — Feature Engineering
</h2>

<div class="qa-section">

  <div class="qa-block">
    <button>1. What is feature engineering in machine learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Feature engineering is the process of creating, transforming, or selecting
      input features so that machine learning models can learn patterns more effectively.
      In practice, good features matter more than complex algorithms.
    </div>
  </div>

  <div class="qa-block">
    <button>2. Why is feature engineering more important than choosing algorithms? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Algorithms learn only from the information provided to them.
      Well-designed features expose patterns clearly, allowing even simple models
      to perform well, whereas poor features limit any algorithm’s performance.
    </div>
  </div>

  <div class="qa-block">
    <button>3. What are common types of feature engineering techniques? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Common techniques include feature scaling, encoding categorical variables,
      handling missing values, creating interaction features,
      binning, and extracting time-based features.
    </div>
  </div>

  <div class="qa-block">
    <button>4. Why is feature scaling important? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Feature scaling ensures that features contribute equally to the model.
      It is especially important for distance-based and gradient-based algorithms
      such as KNN, SVM, and linear regression.
    </div>
  </div>

  <div class="qa-block">
    <button>5. What is the difference between normalization and standardization? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Normalization rescales data to a fixed range, usually 0 to 1.
      Standardization transforms data to have mean 0 and standard deviation 1.
      The choice depends on the algorithm and data distribution.
    </div>
  </div>

  <div class="qa-block">
    <button>6. How do you handle categorical variables in machine learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Categorical variables can be handled using techniques such as
      label encoding, one-hot encoding, target encoding,
      or frequency encoding depending on cardinality and use case.
    </div>
  </div>

  <div class="qa-block">
    <button>7. What problems can one-hot encoding cause? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      One-hot encoding can significantly increase dimensionality,
      leading to sparse data, higher memory usage,
      and potential overfitting for high-cardinality features.
    </div>
  </div>

  <div class="qa-block">
    <button>8. How do you handle missing values during feature engineering? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Missing values can be handled by deletion, mean/median/mode imputation,
      model-based imputation, or by creating a separate “missing” indicator feature.
    </div>
  </div>

  <div class="qa-block">
    <button>9. What is feature interaction and when is it useful? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Feature interaction combines two or more features to capture relationships
      that individual features cannot represent alone.
      It is especially useful for linear models.
    </div>
  </div>

  <div class="qa-block">
    <button>10. How do tree-based models reduce the need for feature engineering? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Tree-based models automatically learn non-linear relationships
      and feature interactions, reducing the need for extensive scaling
      or manual interaction features.
    </div>
  </div>

  <div class="qa-block">
    <button>11. What is feature selection and why is it important? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Feature selection removes irrelevant or redundant features
      to reduce overfitting, improve interpretability,
      and speed up model training.
    </div>
  </div>

  <div class="qa-block">
    <button>12. How do you know if a feature is useful? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      A feature is useful if it improves validation performance,
      reduces error, or adds meaningful predictive signal.
      Feature importance, correlation analysis, and ablation studies help assess this.
    </div>
  </div>

</div>
<!-- =================================================
     TOPIC — MODEL EVALUATION & METRICS
================================================= -->
<h2 class="topic-header topic-slate">
  Topic — Model Evaluation & Metrics
</h2>

<div class="qa-section">

  <div class="qa-block">
    <button>1. Why is model evaluation important in machine learning? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Model evaluation tells us how well a model will perform on unseen data.
      Without proper evaluation, a model may look good during training
      but fail completely in real-world usage.
    </div>
  </div>

  <div class="qa-block">
    <button>2. What is the difference between training error and test error? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Training error measures performance on data the model has already seen.
      Test error measures performance on unseen data and reflects true generalization.
    </div>
  </div>

  <div class="qa-block">
    <button>3. Why is accuracy not always a good evaluation metric? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Accuracy can be misleading in imbalanced datasets.
      A model predicting only the majority class can achieve high accuracy
      while being useless for the actual business problem.
    </div>
  </div>

  <div class="qa-block">
    <button>4. What is a confusion matrix and why is it useful? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      A confusion matrix shows counts of true positives, true negatives,
      false positives, and false negatives, helping understand
      the types of errors a model makes.
    </div>
  </div>

  <div class="qa-block">
    <button>5. What is precision and when is it important? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Precision measures how many predicted positives are actually correct.
      It is important when false positives are costly,
      such as in spam filtering or fraud alerts.
    </div>
  </div>

  <div class="qa-block">
    <button>6. What is recall and when is it more important than precision? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Recall measures how many actual positives were correctly identified.
      It is crucial when missing a positive case is expensive,
      such as disease detection or fraud prevention.
    </div>
  </div>

  <div class="qa-block">
    <button>7. What is the F1-score? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      The F1-score is the harmonic mean of precision and recall.
      It is useful when there is an imbalance between classes
      and both false positives and false negatives matter.
    </div>
  </div>

  <div class="qa-block">
    <button>8. What is ROC curve and AUC? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      The ROC curve plots true positive rate against false positive rate.
      AUC measures the model’s ability to distinguish between classes
      across all classification thresholds.
    </div>
  </div>

  <div class="qa-block">
    <button>9. Why can ROC-AUC be misleading in some cases? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      ROC-AUC does not account for class imbalance or business costs.
      A model with good AUC may still perform poorly at the chosen decision threshold.
    </div>
  </div>

  <div class="qa-block">
    <button>10. What evaluation metrics are used for regression problems? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Common regression metrics include Mean Absolute Error (MAE),
      Mean Squared Error (MSE), Root Mean Squared Error (RMSE),
      and R-squared.
    </div>
  </div>

  <div class="qa-block">
    <button>11. MAE vs RMSE — when do you prefer each? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      MAE treats all errors equally and is robust to outliers.
      RMSE penalizes large errors more heavily and is preferred
      when large deviations are especially undesirable.
    </div>
  </div>

  <div class="qa-block">
    <button>12. How do business requirements influence metric selection? <i class="fas fa-chevron-down"></i></button>
    <div class="content">
      Metrics must align with business impact.
      For example, in fraud detection recall may matter more than accuracy,
      while in pricing models minimizing large errors may be critical.
    </div>
  </div>

</div>


</div>

<!-- Back Button -->
<div style="text-align:center; margin:2rem 0;">
  <button onclick="history.back()" class="back-btn">← Go Back</button>
</div>

</main>

<!-- ================= FOOTER ================= -->
<footer class="site-footer">
  <div class="site-container footer-inner">
    <p>© 2025 Nisa — Built with ❤️ by Nisa</p>
    <div class="socials">
      <a href="#"><i class="fab fa-linkedin"></i></a>
      <a href="#"><i class="fab fa-github"></i></a>
      <a href="#"><i class="fab fa-medium"></i></a>
      <a href="#"><i class="fab fa-youtube"></i></a>
    </div>
  </div>
</footer>

<!-- ================= COLLAPSE SCRIPT ================= -->
<script>
document.querySelectorAll('.qa-block button').forEach(btn=>{
  btn.addEventListener('click',()=>{
    const content=btn.nextElementSibling;
    const icon=btn.querySelector('i');
    const open=content.style.display==='block';

    document.querySelectorAll('.qa-block .content').forEach(c=>c.style.display='none');
    document.querySelectorAll('.qa-block i').forEach(i=>{
      i.classList.remove('fa-chevron-up');
      i.classList.add('fa-chevron-down');
    });

    if(!open){
      content.style.display='block';
      icon.classList.remove('fa-chevron-down');
      icon.classList.add('fa-chevron-up');
    }
  });
});
</script>

</body>
</html>
